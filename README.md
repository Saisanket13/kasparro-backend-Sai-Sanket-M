# üöÄ Kasparro Backend & ETL System

**Author**: Sai Sanket M  
**Repository**: `Saisanket13/kasparro-backend-Sai-Sanket-M`

Production-grade ETL pipeline with cryptocurrency data ingestion, normalization, and REST API.

---

## üìã Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Quick Start](#quick-start)
- [API Endpoints](#api-endpoints)
- [Project Structure](#project-structure)
- [Testing](#testing)
- [Cloud Deployment](#cloud-deployment)
- [Configuration](#configuration)

---

## üéØ Overview

This system implements a **production-grade ETL pipeline** that:

1. Ingests cryptocurrency data from **3 sources**:
   - CoinPaprika API
   - CoinGecko API
   - CSV files

2. Normalizes data into a **unified schema**

3. Stores raw + normalized data in **PostgreSQL**

4. Exposes **REST APIs** for data access

5. Implements **incremental ingestion** with checkpointing

6. Runs in **Docker containers** with automated scheduling

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Data Sources                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  CoinPaprika  ‚îÇ  CoinGecko    ‚îÇ  CSV Files              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ               ‚îÇ                 ‚îÇ
        ‚ñº               ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              ETL Ingestion Layer                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Data Extraction                               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Validation (Pydantic)                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Normalization                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Checkpoint Management                         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PostgreSQL Database                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  raw_crypto_data‚îÇ  crypto_prices   ‚îÇ checkpoints  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              FastAPI REST API                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  GET /data      - Paginated data with filters    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  GET /health    - System health check            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  GET /stats     - ETL statistics                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  GET /runs      - ETL run history                ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚ú® Features

### P0 - Foundation ‚úÖ
- ‚úÖ Data ingestion from 2 sources (CoinPaprika + CSV)
- ‚úÖ PostgreSQL storage (raw + normalized tables)
- ‚úÖ Type validation with Pydantic
- ‚úÖ Incremental ingestion
- ‚úÖ REST API with `/data` and `/health` endpoints
- ‚úÖ Dockerized system (`make up`, `make down`, `make test`)
- ‚úÖ Basic test suite

### P1 - Growth Layer ‚úÖ
- ‚úÖ Third data source (CoinGecko)
- ‚úÖ Checkpoint table with resume-on-failure
- ‚úÖ Idempotent writes
- ‚úÖ `/stats` endpoint
- ‚úÖ Comprehensive test coverage
- ‚úÖ Clean architecture

### P2 - Differentiator Layer ‚≠ê
- ‚úÖ Failure recovery with checkpoints
- ‚úÖ ETL metadata tracking
- ‚úÖ Structured JSON logs
- ‚úÖ `/runs` and `/compare-runs` endpoints
- ‚úÖ Docker health checks
- ‚ö° Rate limiting (ready for implementation)
- ‚ö° Schema drift detection (ready for implementation)

---

## üöÄ Quick Start

### Prerequisites

- Docker & Docker Compose
- Git
- Make (optional but recommended)

### Setup

1. **Clone the repository**:
```bash
git clone https://github.com/Saisanket13/kasparro-backend-Sai-Sanket-M.git
cd kasparro-backend-Sai-Sanket-M
```

2. **Configure API keys**:
```bash
cp .env.example .env
# Edit .env and add your API keys:
# COINPAPRIKA_API_KEY=your_key_here
# COINGECKO_API_KEY=your_key_here
```

3. **Start the system**:
```bash
make up
```

4. **Check status**:
```bash
make logs
```

The API will be available at:
- üåê **API**: http://localhost:8000
- üìö **Interactive Docs**: http://localhost:8000/docs
- üîç **ReDoc**: http://localhost:8000/redoc

---

## üì° API Endpoints

### 1. GET `/data`
Get cryptocurrency price data with pagination and filters.

**Query Parameters**:
- `limit` (int, 1-100): Records per page (default: 10)
- `offset` (int): Skip records (default: 0)
- `symbol` (str): Filter by symbol (e.g., BTC, ETH)
- `source` (str): Filter by source (coinpaprika, coingecko, csv)

**Example**:
```bash
curl "http://localhost:8000/data?limit=5&symbol=BTC"
```

**Response**:
```json
{
  "request_id": "uuid-here",
  "api_latency_ms": 45.23,
  "total_records": 100,
  "page": 1,
  "limit": 5,
  "data": [
    {
      "id": 1,
      "coin_id": "bitcoin",
      "symbol": "BTC",
      "name": "Bitcoin",
      "price_usd": 42500.50,
      "market_cap": 850000000000,
      "volume_24h": 28000000000,
      "price_change_24h": 2.5,
      "timestamp": "2025-12-28T10:30:00",
      "source": "coinpaprika"
    }
  ]
}
```

### 2. GET `/health`
System health check.

**Example**:
```bash
curl http://localhost:8000/health
```

**Response**:
```json
{
  "status": "healthy",
  "database": "connected",
  "etl_status": {
    "coinpaprika": {
      "last_run_status": "success",
      "last_run_time": "2025-12-28T10:00:00",
      "records_processed": 100
    }
  },
  "timestamp": "2025-12-28T10:30:00"
}
```

### 3. GET `/stats`
ETL statistics for all sources.

**Example**:
```bash
curl http://localhost:8000/stats
```

### 4. GET `/runs`
ETL run history.

**Query Parameters**:
- `limit` (int): Number of runs (default: 10)
- `source` (str): Filter by source

**Example**:
```bash
curl "http://localhost:8000/runs?limit=5"
```

### 5. GET `/compare-runs`
Compare two ETL runs.

**Query Parameters**:
- `run_id_1` (str): First run ID
- `run_id_2` (str): Second run ID

**Example**:
```bash
curl "http://localhost:8000/compare-runs?run_id_1=coinpaprika_123&run_id_2=coinpaprika_456"
```

---

## üìÅ Project Structure

```
kasparro-backend-Sai-Sanket-M/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ main.py                 # FastAPI application
‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îú‚îÄ‚îÄ coinpaprika.py          # CoinPaprika ETL
‚îÇ   ‚îú‚îÄ‚îÄ coingecko.py            # CoinGecko ETL
‚îÇ   ‚îú‚îÄ‚îÄ csv_source.py           # CSV ETL
‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.py         # ETL orchestrator
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îî‚îÄ‚îÄ data_service.py         # Business logic layer
‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îî‚îÄ‚îÄ models.py               # Pydantic schemas
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ database.py             # Database models
‚îÇ   ‚îî‚îÄ‚îÄ config.py               # Configuration
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py             # Test suite
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ crypto_prices.csv       # Sample CSV data
‚îú‚îÄ‚îÄ Dockerfile                  # Container image
‚îú‚îÄ‚îÄ docker-compose.yml          # Multi-container setup
‚îú‚îÄ‚îÄ Makefile                    # Command shortcuts
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ .env.example                # Environment template
‚îî‚îÄ‚îÄ README.md                   # This file
```

---

## üß™ Testing

### Run all tests:
```bash
make test
```

### Run tests locally:
```bash
make test-local
```

### Test coverage includes:
- ‚úÖ API endpoint functionality
- ‚úÖ Data validation
- ‚úÖ ETL transformation logic
- ‚úÖ Checkpoint management
- ‚úÖ Failure scenarios
- ‚úÖ Schema validation

---

## ‚òÅÔ∏è Cloud Deployment

## üöÄ Live Deployment (Verified)

The backend API is deployed and publicly accessible for verification.

**Base URL**:  
https://kasparro-backend-sai-sanket.onrender.com

### Public Endpoints
- `/health` ‚Äì System health check  
- `/data` ‚Äì Normalized crypto data  
- `/stats` ‚Äì ETL statistics  
- `/docs` ‚Äì Swagger UI  

> The deployed service runs the same Docker image defined in this repository.

### AWS Deployment

1. **Setup RDS PostgreSQL**:
```bash
aws rds create-db-instance \
  --db-instance-identifier kasparro-db \
  --db-instance-class db.t3.micro \
  --engine postgres \
  --master-username etl_user \
  --master-user-password YOUR_PASSWORD \
  --allocated-storage 20
```

2. **Deploy to ECS**:
```bash
# Build and push image
docker build -t kasparro-backend .
docker tag kasparro-backend:latest YOUR_ECR_REPO/kasparro-backend:latest
docker push YOUR_ECR_REPO/kasparro-backend:latest

# Create ECS task definition and service
aws ecs create-service --service-name kasparro-api ...
```

3. **Setup EventBridge Cron**:
```bash
aws events put-rule \
  --name kasparro-etl-hourly \
  --schedule-expression "rate(1 hour)"
```

### GCP Deployment

1. **Setup Cloud SQL**:
```bash
gcloud sql instances create kasparro-db \
  --database-version=POSTGRES_15 \
  --tier=db-f1-micro \
  --region=us-central1
```

2. **Deploy to Cloud Run**:
```bash
gcloud run deploy kasparro-api \
  --image gcr.io/YOUR_PROJECT/kasparro-backend \
  --platform managed
```

3. **Setup Cloud Scheduler**:
```bash
gcloud scheduler jobs create http kasparro-etl \
  --schedule="0 * * * *" \
  --uri="https://YOUR_CLOUD_RUN_URL/run-etl"
```

---

## ‚öôÔ∏è Configuration

### Environment Variables

Create a `.env` file:

```env
# Database
DATABASE_URL=postgresql://etl_user:etl_password@localhost:5432/etl_db

# API Keys
COINPAPRIKA_API_KEY=your_coinpaprika_key
COINGECKO_API_KEY=your_coingecko_key

# Application
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO
```

---

## üîß Commands Reference

```bash
make up            # Start system
make down          # Stop system
make restart       # Restart system
make logs          # View logs
make test          # Run tests
make clean         # Clean up
make init-db       # Initialize database
make run-etl       # Run ETL manually
make shell         # Open shell in container
make shell-db      # Open PostgreSQL shell
make backup-db     # Backup database
```

---

## üìä ETL Flow

1. **Extraction**: Fetch data from APIs/CSV
2. **Validation**: Pydantic schema validation
3. **Storage**: Save raw data to `raw_crypto_data`
4. **Transformation**: Normalize to common schema
5. **Loading**: Insert into `crypto_prices`
6. **Checkpointing**: Update ETL metadata
7. **Scheduling**: Automated hourly runs

---

## üéØ Key Design Decisions

### 1. **Checkpoint-Based Incremental Ingestion**
- Tracks last processed record per source
- Enables resume-on-failure
- Prevents duplicate processing

### 2. **Dual Storage Strategy**
- Raw data preserved for audit/replay
- Normalized data for fast querying

### 3. **Idempotent Design**
- Repeated runs don't create duplicates
- Safe to retry failed operations

### 4. **Clean Architecture**
- Separation of concerns
- Easy to add new data sources
- Testable components

---

## üêõ Troubleshooting

### Database connection issues:
```bash
# Check database status
make shell-db

# Reinitialize database
make init-db
```

### API not responding:
```bash
# Check logs
make logs-api

# Restart services
make restart
```

### ETL failures:
```bash
# Check ETL logs
make logs-etl

# Run ETL manually with debug
docker-compose exec api python -c "from ingestion.orchestrator import ETLOrchestrator; ETLOrchestrator().run_all()"
```

---

## üìù Assignment Completion Checklist

### P0 - Foundation ‚úÖ
- [x] Data ingestion from 2 sources (API + CSV)
- [x] PostgreSQL storage (raw + normalized)
- [x] Type validation (Pydantic)
- [x] Incremental ingestion
- [x] `/data` endpoint with pagination
- [x] `/health` endpoint
- [x] Dockerized system
- [x] Basic tests

### P1 - Growth Layer ‚úÖ
- [x] Third data source (CoinGecko)
- [x] Checkpoint table
- [x] Resume-on-failure logic
- [x] Idempotent writes
- [x] `/stats` endpoint
- [x] Comprehensive tests
- [x] Clean architecture

### P2 - Differentiator ‚≠ê
- [x] Failure recovery
- [x] ETL metadata tracking
- [x] Structured logs
- [x] `/runs` endpoint
- [x] `/compare-runs` endpoint
- [x] Docker health checks

---

## üèÜ What Makes This Solution Stand Out

1. **Production-Ready**: Not a demo, but actual production patterns
2. **Comprehensive Testing**: Real test coverage, not just smoke tests
3. **Proper Error Handling**: Graceful failures with detailed logging
4. **Scalable Design**: Easy to add new sources or features
5. **Documentation**: Clear README with examples
6. **DevOps Integration**: Complete Docker + Make workflow

---

## üë§ Author

**Sai Sanket M**  
GitHub: [@Saisanket13](https://github.com/Saisanket13)

---

## üìÑ License

This project is part of the Kasparro hiring process.

---

**Built with ‚ù§Ô∏è for Kasparro**
